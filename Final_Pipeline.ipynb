{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6abe400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Data Collection\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import logging\n",
    "import urllib\n",
    "import json\n",
    "import datetime as dt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "\n",
    "#####################################################################\n",
    "class aqms_api_class(object):\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(\"aqms_logger\")\n",
    "        self.url_api = \"https://data.airquality.nsw.gov.au\"\n",
    "        self.headers = {'content-type': 'application/json', 'accept': 'application/json'}\n",
    "        self.get_observations = 'api/Data/get_Observations'\n",
    "\n",
    "    def get_Obs(self, ObsRequest):\n",
    "        query = urllib.parse.urljoin(self.url_api, self.get_observations)\n",
    "        response = requests.post(url=query, data=json.dumps(ObsRequest), headers=self.headers)\n",
    "        return response\n",
    "\n",
    "#####################################################################\n",
    "def ObsRequest_init(site_id, start_date, end_date):\n",
    "    ObsRequest = {}\n",
    "    ObsRequest['Parameters'] = [\n",
    "        'PM10', 'PM2.5', 'NO2', 'CO', 'OZONE',\n",
    "        'WSP', 'WDR', 'SD1', 'TEMP', 'HUMID'\n",
    "    ]\n",
    "    ObsRequest['Sites'] = [site_id]\n",
    "    ObsRequest['StartDate'] = start_date.strftime('%Y-%m-%d')\n",
    "    ObsRequest['EndDate'] = end_date.strftime('%Y-%m-%d')\n",
    "    ObsRequest['Categories'] = ['Averages']\n",
    "    ObsRequest['SubCategories'] = ['Hourly']\n",
    "    ObsRequest['Frequency'] = ['Hourly average']\n",
    "    return ObsRequest\n",
    "\n",
    "#####################################################################\n",
    "if __name__ == '__main__':\n",
    "    AQMS = aqms_api_class()\n",
    "\n",
    "    site_ids = [39, 1141, 919, 2560, 107]\n",
    "    start_date = dt.date(2015 , 6, 1)\n",
    "    end_date = dt.date(2025, 6, 1)\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    os.makedirs(\"batches\", exist_ok=True)\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_month = current_date + relativedelta(months=1)\n",
    "\n",
    "        for site_id in site_ids:\n",
    "            print(f\"ðŸ“¡ Fetching Site {site_id} from {current_date} to {next_month}\")\n",
    "            ObsRequest = ObsRequest_init(site_id, current_date, next_month)\n",
    "            try:\n",
    "                response = AQMS.get_Obs(ObsRequest)\n",
    "                filename = f\"batches/site_{site_id}_{current_date}.json\"\n",
    "\n",
    "                with open(filename, 'w', encoding='utf-8') as f:\n",
    "                    f.write(response.text)\n",
    "                print(f\" Saved to {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\" Failed for Site {site_id} on {current_date}: {e}\")\n",
    "\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        current_date = next_month\n",
    "\n",
    "\n",
    "# STEP 1.5: Reduce Irrelevant Metadata\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"combined.csv\")\n",
    "\n",
    "columns_to_drop = [\n",
    "    \"AirQualityCategory\",\n",
    "    \"DeterminingPollutant\",\n",
    "    \"Parameter.ParameterDescription\",\n",
    "    \"Parameter.UnitsDescription\",\n",
    "    \"Parameter.Category\",\n",
    "    \"Parameter.SubCategory\",\n",
    "    \"Parameter.Frequency\"\n",
    "]\n",
    "df_reduced = df.drop(columns=columns_to_drop)\n",
    "\n",
    "df_reduced.to_csv(\"full_relevant_features.csv\", index=False)\n",
    "print(\" Saved reduced dataset as Full_Relevant\")\n",
    "\n",
    "\n",
    "# STEP 2: Post-Acquisition Processing (Pivot, Fill, Checks)\n",
    "\n",
    "import pandas as pd\n",
    "df=pd.read_csv(\"combined.csv\")\n",
    "df['ParsedDate'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')\n",
    "df['StartHourStr'] = df['HourDescription'].str.extract(r'(^[\\d]+ ?[ap]m)', expand=False)\n",
    "df['HourInt'] = pd.to_datetime(df['StartHourStr'], format='%I %p', errors='coerce').dt.hour\n",
    "df['Timestamp'] = df['ParsedDate'] + pd.to_timedelta(df['HourInt'], unit='h')\n",
    "df = df.dropna(subset=['Timestamp'])\n",
    "pivoted_df = df.pivot_table(\n",
    "    index=['Site_Id', 'Timestamp'],\n",
    "    columns='Parameter.ParameterCode',\n",
    "    values='Value',\n",
    "    aggfunc='first'\n",
    ").reset_index()\n",
    "pivoted_df.columns.name = None\n",
    "pivoted_df.to_csv(\"finalfiles/simplified.csv\", index=False)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"finalfiles/simplified.csv\", parse_dates=[\"Timestamp\"])\n",
    "df = df.sort_values([\"Site_Id\", \"Timestamp\"])\n",
    "df_filled = df.groupby(\"Site_Id\").apply(lambda group: group.ffill()).reset_index(drop=True)\n",
    "df_filled.to_csv(\"finalfiles/cleaned.csv\", index=False)\n",
    "print(\" Forward-filled dataset saved as 'finalfiles/cleaned.csv'\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"finalfiles/2017_cleaned.csv\", parse_dates=[\"Timestamp\"])\n",
    "print(f\"#Dataset Shape: {df.shape}\")\n",
    "print(\"#Missing Values per Column: (Already performeed ffill)\")\n",
    "print(df.isna().sum())\n",
    "df.describe()\n",
    "df.info()\n",
    "\n",
    "\n",
    "# STEP 3: Modelling â€“ XGBoost Variation 1 (Baseline)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# df = pd.read_csv(\"dataset.csv\", parse_dates=['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "features = ['TEMP', 'HUMID', 'WSP', 'WDR', 'SD1']\n",
    "target = 'PM2.5'\n",
    "df = df.dropna(subset=[target])\n",
    "site_scores = []\n",
    "for site in df['Site_Id'].unique():\n",
    "    df_site = df[df['Site_Id'] == site].copy()\n",
    "    df_site = df_site[features + [target]].fillna(method='ffill')\n",
    "    if len(df_site) < 100:\n",
    "        continue\n",
    "    split_idx = int(len(df_site) * 0.8)\n",
    "    train = df_site.iloc[:split_idx]\n",
    "    test = df_site.iloc[split_idx:]\n",
    "    X_train, y_train = train[features], train[target]\n",
    "    X_test, y_test = test[features], test[target]\n",
    "    X_train = X_train[~y_train.isna()]\n",
    "    y_train = y_train.dropna()\n",
    "    X_test = X_test[~y_test.isna()]\n",
    "    y_test = y_test.dropna()\n",
    "    if len(X_train) < 50 or len(X_test) < 10:\n",
    "        continue\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "    ])\n",
    "    param_grid = {\n",
    "        'xgb__n_estimators': [100],\n",
    "        'xgb__learning_rate': [0.1],\n",
    "        'xgb__max_depth': [3],\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    site_scores.append({'Site': site, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
    "    if site == df['Site_Id'].unique()[0]:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.plot(y_test.index, y_test, label='Actual PM2.5', color='blue')\n",
    "        plt.plot(y_test.index, y_pred, label='Predicted PM2.5', color='orange')\n",
    "        plt.title(f'Site {site} - Actual vs Predicted PM2.5')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('PM2.5')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "results_df = pd.DataFrame(site_scores)\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "# STEP 3: Modelling â€“ XGBoost Variation 2 (Log Transform)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "df = df[df['PM2.5'] > 0]\n",
    "df['PM2.5'] = np.log1p(df['PM2.5'])\n",
    "features = ['TEMP', 'HUMID', 'WSP', 'WDR', 'SD1']\n",
    "target = 'PM2.5'\n",
    "results = []\n",
    "for site in df['Site_Id'].unique():\n",
    "    df_site = df[df['Site_Id'] == site]\n",
    "    if len(df_site) < 100:\n",
    "        continue\n",
    "    X = df_site[features]\n",
    "    y = df_site[target]\n",
    "    split_idx = int(len(df_site) * 0.8)\n",
    "    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "    ])\n",
    "    param_grid = {\n",
    "        'xgb__n_estimators': [100],\n",
    "        'xgb__learning_rate': [0.1],\n",
    "        'xgb__max_depth': [3],\n",
    "        'xgb__subsample': [0.8],\n",
    "        'xgb__colsample_bytree': [0.8]\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=tscv, scoring='r2', n_jobs=-1, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.predict(X_test)\n",
    "    y_test_inv = np.expm1(y_test)\n",
    "    y_pred_inv = np.expm1(y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "    r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "    results.append({\n",
    "        'Site': site,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    })\n",
    "result_df = pd.DataFrame(results)\n",
    "print(result_df.sort_values(by='Site'))\n",
    "\n",
    "\n",
    "# STEP 3: Modelling â€“ XGBoost Variation 3 (Wider Grid)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "features = ['TEMP', 'HUMID', 'WSP', 'WDR', 'SD1']\n",
    "target = 'PM2.5'\n",
    "df = df.dropna(subset=[target] + features)\n",
    "df = df[df[target] > 0]\n",
    "df['PM2.5'] = np.log1p(df['PM2.5'])\n",
    "sites = df['Site_Id'].unique()\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': [100, 200],\n",
    "    'xgb__learning_rate': [0.01, 0.1],\n",
    "    'xgb__max_depth': [3, 5],\n",
    "    'xgb__subsample': [0.8, 1.0],\n",
    "    'xgb__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "results = []\n",
    "for site in sites:\n",
    "    df_site = df[df['Site_Id'] == site].copy()\n",
    "    df_site = df_site.sort_values('Timestamp')\n",
    "    X = df_site[features]\n",
    "    y = df_site[target]\n",
    "    split = int(0.8 * len(df_site))\n",
    "    X_train, X_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    X_train = X_train[~y_train.isna()]\n",
    "    y_train = y_train.dropna()\n",
    "    X_test = X_test[~y_test.isna()]\n",
    "    y_test = y_test.dropna()\n",
    "    pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('xgb', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "    ])\n",
    "    grid = GridSearchCV(pipe, param_grid, cv=tscv, scoring='r2', n_jobs=-1, verbose=0)\n",
    "    grid.fit(X_train, y_train)\n",
    "    y_pred = grid.predict(X_test)\n",
    "    y_test_inv = np.expm1(y_test)\n",
    "    y_pred_inv = np.expm1(y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))\n",
    "    mae = mean_absolute_error(y_test_inv, y_pred_inv)\n",
    "    r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "    results.append({\n",
    "        'Site': site,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'BestParams': grid.best_params_\n",
    "    })\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results[['Site', 'RMSE', 'MAE', 'R2']])\n",
    "\n",
    "\n",
    "# STEP 3: Modelling â€“ Random Forest Variation 1\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv('dataset.csv', parse_dates=['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "df = df[df['PM2.5'] > 0].dropna()\n",
    "df['PM2.5'] = np.log1p(df['PM2.5'])\n",
    "features = ['TEMP', 'HUMID', 'WSP', 'WDR', 'SD1']\n",
    "target = 'PM2.5'\n",
    "results = []\n",
    "for site in df['Site_Id'].unique():\n",
    "    df_site = df[df['Site_Id'] == site].copy()\n",
    "    df_site = df_site[features + [target]].dropna()\n",
    "    split_idx = int(len(df_site) * 0.8)\n",
    "    train = df_site.iloc[:split_idx]\n",
    "    test = df_site.iloc[split_idx:]\n",
    "    X_train, y_train = train[features], train[target]\n",
    "    X_test, y_test = test[features], test[target]\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    grid = GridSearchCV(model, param_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_exp = np.expm1(y_pred)\n",
    "    y_test_exp = np.expm1(y_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_exp, y_pred_exp))\n",
    "    mae = mean_absolute_error(y_test_exp, y_pred_exp)\n",
    "    r2 = r2_score(y_test_exp, y_pred_exp)\n",
    "    results.append({'Site': site, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n",
    "\n",
    "\n",
    "# STEP 3: Modelling â€“ Linear Regression Variations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "feature_cols = ['TEMP', 'HUMID', 'WSP', 'WDR', 'SD1']\n",
    "target_col = 'PM2.5'\n",
    "df = df.dropna(subset=feature_cols + [target_col])\n",
    "df = df[df[target_col] > 0]\n",
    "site_metrics = []\n",
    "for site_id in df['Site_Id'].unique():\n",
    "    site_df = df[df['Site_Id'] == site_id].copy()\n",
    "    site_df.sort_index(inplace=True)\n",
    "    split_idx = int(len(site_df) * 0.8)\n",
    "    train_df = site_df.iloc[:split_idx]\n",
    "    test_df = site_df.iloc[split_idx:]\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[target_col]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    site_metrics.append({\n",
    "        'Site': site_id,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    })\n",
    "lr_results = pd.DataFrame(site_metrics)\n",
    "print(lr_results)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "feature_cols = ['TEMP', 'HUMID', 'WSP', 'WDR', 'SD1']\n",
    "target_col = 'PM2.5'\n",
    "df = df.dropna(subset=feature_cols + [target_col])\n",
    "df = df[df[target_col] > 0]\n",
    "site_metrics = []\n",
    "for site_id in df['Site_Id'].unique():\n",
    "    site_df = df[df['Site_Id'] == site_id].copy()\n",
    "    site_df.sort_index(inplace=True)\n",
    "    split_idx = int(len(site_df) * 0.8)\n",
    "    train_df = site_df.iloc[:split_idx]\n",
    "    test_df = site_df.iloc[split_idx:]\n",
    "    X_train = train_df[feature_cols]\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df[feature_cols]\n",
    "    y_test = test_df[target_col]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    site_metrics.append({\n",
    "        'Site': site_id,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    })\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(y_test.index, y_test.values, label='Actual', linewidth=2)\n",
    "    plt.plot(y_test.index, y_pred, label='Predicted', linewidth=2)\n",
    "    plt.title(f'Site {site_id} â€“ Predicted vs Actual PM2.5')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('PM2.5 Concentration')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "lr_results = pd.DataFrame(site_metrics)\n",
    "print(lr_results)\n",
    "\n",
    "\n",
    "# STEP 3: Modelling â€“ Random Forest Variation 2 (With Plots)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "df = pd.read_csv('dataset.csv', parse_dates=['Timestamp'])\n",
    "df.set_index('Timestamp', inplace=True)\n",
    "df = df[df['PM2.5'] > 0].dropna()\n",
    "df['PM2.5'] = np.log1p(df['PM2.5'])\n",
    "features = ['TEMP', 'HUMID', 'WSP', 'WDR', 'SD1']\n",
    "target = 'PM2.5'\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "results = []\n",
    "for site in df['Site_Id'].unique():\n",
    "    df_site = df[df['Site_Id'] == site].copy().sort_index()\n",
    "    df_site = df_site[features + [target]].dropna()\n",
    "    split_idx = int(len(df_site) * 0.8)\n",
    "    train = df_site.iloc[:split_idx]\n",
    "    test = df_site.iloc[split_idx:]\n",
    "    X_train, y_train = train[features], train[target]\n",
    "    X_test, y_test = test[features], test[target]\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [5, 10, None],\n",
    "        'min_samples_split': [2, 5]\n",
    "    }\n",
    "    grid = GridSearchCV(model, param_grid, cv=tscv, scoring='r2', n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_exp = np.expm1(y_pred)\n",
    "    y_test_exp = np.expm1(y_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_exp, y_pred_exp))\n",
    "    mae = mean_absolute_error(y_test_exp, y_pred_exp)\n",
    "    r2 = r2_score(y_test_exp, y_pred_exp)\n",
    "    results.append({'Site': site, 'RMSE': rmse, 'MAE': mae, 'R2': r2})\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(y_test_exp.index, y_test_exp.values, label='Actual', linewidth=2)\n",
    "    plt.plot(y_test_exp.index, y_pred_exp, label='Predicted', linewidth=2)\n",
    "    plt.title(f'Site {site} â€“ Random Forest: Predicted vs Actual PM2.5')\n",
    "    plt.xlabel('Timestamp')\n",
    "    plt.ylabel('PM2.5 Concentration')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
